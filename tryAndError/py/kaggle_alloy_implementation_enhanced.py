"""
Kaggle AI-Augmentedå½¢å¼æ‰‹æ³• - æ‹¡å¼µç‰ˆï¼ˆç‰¹å¾´é‡è¿½åŠ ï¼‰

Alloyãƒ¢ãƒ‡ãƒ«: kaggle_competition_v3_final.als
æ¤œè¨¼çµæœ:
  âœ… G1_Achieved: ãƒ‡ãƒ¼ã‚¿å“è³ªä¿è¨¼
  âœ… G2_Achieved: ç‰¹å¾´é‡ã‚¨ãƒ³ã‚¸ãƒ‹ã‚¢ãƒªãƒ³ã‚°ï¼ˆæ‹¡å¼µç‰ˆï¼‰
  âœ… G3_Achieved: ãƒ¢ãƒ‡ãƒ«æ§‹ç¯‰
  âš ï¸ G4_Achieved: å®Ÿè£…ãƒ¬ãƒ™ãƒ«ã§è¿½åŠ ï¼ˆAlloyæ¤œè¨¼ãªã—ï¼‰

å¯¾è±¡: ã‚¿ã‚¤ã‚¿ãƒ‹ãƒƒã‚¯ç”Ÿå­˜äºˆæ¸¬
è¿½åŠ ç‰¹å¾´é‡: Title, Cabin, Sex, Embarked
"""

import pandas as pd
import numpy as np
from sklearn.ensemble import RandomForestClassifier
from sklearn.model_selection import train_test_split, cross_val_score
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score
from typing import Dict, List, Tuple, Optional
import warnings
warnings.filterwarnings('ignore')


class AlloyConstraintValidator:
    """
    Alloyå½¢å¼è¨˜æ³•ã§å®šç¾©ã•ã‚ŒãŸåˆ¶ç´„ã‚’Pythonã§æ¤œè¨¼
    
    å¯¾å¿œã™ã‚‹Alloyãƒ¢ãƒ‡ãƒ«: kaggle_competition_v3_final.als
    æ¤œè¨¼æ¸ˆã¿ã‚´ãƒ¼ãƒ«: G1, G2, G3
    """
    
    @staticmethod
    def validate_no_missing(df: pd.DataFrame, stage: str = "") -> bool:
        """
        Alloy: fact MissingValueHandling
        Alloy: fact NumericalMissingValues
        Alloy: fact CategoricalMissingValues
        
        åˆ¶ç´„: å‰å‡¦ç†å¾Œã®ãƒ‡ãƒ¼ã‚¿ã«ã¯æ¬ æå€¤ãŒãªã„
        """
        missing_count = df.isnull().sum().sum()
        if missing_count > 0:
            raise ValueError(f"[G1é•å] {stage}: æ¬ æå€¤ãŒ{missing_count}å€‹æ®‹ã£ã¦ã„ã¾ã™")
        print(f"âœ… [G1] {stage}: æ¬ æå€¤ãƒã‚§ãƒƒã‚¯åˆæ ¼")
        return True
    
    @staticmethod
    def validate_outliers(df: pd.DataFrame, column: str, min_val: float, max_val: float) -> bool:
        """
        Alloy: fact OutlierConstraints
        åˆ¶ç´„: å€¤ãŒå¦¥å½“ãªç¯„å›²å†…ï¼ˆä¾‹: å¹´é½¢ã¯0-120ï¼‰
        """
        if column not in df.columns:
            return True
            
        actual_min = df[column].min()
        actual_max = df[column].max()
        
        if actual_min < min_val or actual_max > max_val:
            raise ValueError(
                f"[G1é•å] {column}ã®ç¯„å›²åˆ¶ç´„é•å: "
                f"æœŸå¾…[{min_val}, {max_val}], å®Ÿéš›[{actual_min}, {actual_max}]"
            )
        print(f"âœ… [G1] {column}: å¤–ã‚Œå€¤åˆ¶ç´„åˆæ ¼ [{actual_min:.2f}, {actual_max:.2f}]")
        return True
    
    @staticmethod
    def validate_hyperparameters(params: Dict) -> bool:
        """
        Alloy: fact HyperparameterConstraints
        åˆ¶ç´„: ãƒã‚¤ãƒ‘ãƒ¼ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ãŒå¦¥å½“ãªç¯„å›²
        """
        if 'max_depth' in params:
            depth = params['max_depth']
            if not (1 <= depth <= 20):
                raise ValueError(f"[G3é•å] max_depthã¯[1, 20]ã®ç¯„å›²: {depth}")
        
        if 'n_estimators' in params:
            n_est = params['n_estimators']
            if not (1 <= n_est <= 1000):
                raise ValueError(f"[G3é•å] n_estimatorsã¯[1, 1000]ã®ç¯„å›²: {n_est}")
        
        print(f"âœ… [G3] ãƒã‚¤ãƒ‘ãƒ¼ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿æ¤œè¨¼åˆæ ¼: {params}")
        return True
    
    @staticmethod
    def validate_predictions_binary(predictions: np.ndarray) -> bool:
        """
        Alloy: fact PredictionBinary
        åˆ¶ç´„: äºˆæ¸¬å€¤ã¯0ã¾ãŸã¯1
        """
        unique_values = set(predictions)
        if not unique_values.issubset({0, 1}):
            raise ValueError(f"[G3é•å] äºˆæ¸¬å€¤ã¯0ã¾ãŸã¯1ã§ã‚ã‚‹å¿…è¦ãŒã‚ã‚Šã¾ã™: {unique_values}")
        print(f"âœ… [G3] äºˆæ¸¬å€¤ãƒã‚¤ãƒŠãƒªãƒã‚§ãƒƒã‚¯åˆæ ¼")
        return True
    
    @staticmethod
    def validate_new_features_exist(df: pd.DataFrame, expected_features: List[str]) -> bool:
        """
        Alloy: fact FeatureEngineeringRules (simplified)
        åˆ¶ç´„: æ–°ã—ã„ç‰¹å¾´é‡ãŒå­˜åœ¨ã™ã‚‹
        """
        for feature in expected_features:
            if feature not in df.columns:
                raise ValueError(f"[G2é•å] ç‰¹å¾´é‡ {feature} ãŒç”Ÿæˆã•ã‚Œã¦ã„ã¾ã›ã‚“")
        print(f"âœ… [G2] æ–°ç‰¹å¾´é‡æ¤œè¨¼åˆæ ¼: {expected_features}")
        return True
    
    @staticmethod
    def validate_train_test_separation(train_indices: set, test_indices: set) -> bool:
        """
        Alloy: fact TrainTestSeparation
        åˆ¶ç´„: è¨“ç·´ãƒ‡ãƒ¼ã‚¿ã¨ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ã¯é‡è¤‡ã—ãªã„
        """
        overlap = train_indices & test_indices
        if len(overlap) > 0:
            raise ValueError(f"[G1é•å] è¨“ç·´ã¨ãƒ†ã‚¹ãƒˆãŒ{len(overlap)}è¡Œé‡è¤‡ã—ã¦ã„ã¾ã™")
        print(f"âœ… [G1] è¨“ç·´ãƒ»ãƒ†ã‚¹ãƒˆåˆ†é›¢ãƒã‚§ãƒƒã‚¯åˆæ ¼")
        return True


class DataPreprocessor:
    """
    G1: ãƒ‡ãƒ¼ã‚¿å“è³ªä¿è¨¼
    
    å¯¾å¿œã™ã‚‹Alloyè¿°èª: G1_Achieved
    å¯¾å¿œã™ã‚‹KAOS Goal:
    - G11: æ¬ æå€¤å‡¦ç†
    - G12: å¤–ã‚Œå€¤å‡¦ç†
    - G13: ãƒ‡ãƒ¼ã‚¿å‹æ•´åˆæ€§
    """
    
    def __init__(self, validator: AlloyConstraintValidator):
        self.validator = validator
    
    def handle_missing_values(self, df: pd.DataFrame) -> pd.DataFrame:
        """
        G11: æ¬ æå€¤å‡¦ç†
        
        Alloy constraints:
        - fact NumericalMissingValues
        - fact CategoricalMissingValues
        """
        print("\n" + "="*60)
        print("[G11] æ¬ æå€¤å‡¦ç†é–‹å§‹...")
        print("="*60)
        
        df = df.copy()
        
        # æ•°å€¤ç‰¹å¾´é‡ï¼ˆä¸­å¤®å€¤ã§è£œå®Œï¼‰
        numerical_cols = df.select_dtypes(include=['int64', 'float64']).columns
        for col in numerical_cols:
            if df[col].isnull().any():
                median_val = df[col].median()
                df[col].fillna(median_val, inplace=True)
                print(f"  ğŸ“Š {col}: ä¸­å¤®å€¤{median_val:.2f}ã§è£œå®Œ")
        
        # ã‚«ãƒ†ã‚´ãƒªã‚«ãƒ«ç‰¹å¾´é‡ï¼ˆæœ€é »å€¤ã§è£œå®Œï¼‰
        categorical_cols = df.select_dtypes(include=['object']).columns
        for col in categorical_cols:
            if df[col].isnull().any():
                mode_val = df[col].mode()[0] if len(df[col].mode()) > 0 else 'Unknown'
                df[col].fillna(mode_val, inplace=True)
                print(f"  ğŸ“Š {col}: æœ€é »å€¤'{mode_val}'ã§è£œå®Œ")
        
        # Alloyåˆ¶ç´„æ¤œè¨¼
        self.validator.validate_no_missing(df, "æ¬ æå€¤å‡¦ç†å¾Œ")
        
        return df
    
    def handle_outliers(self, df: pd.DataFrame) -> pd.DataFrame:
        """
        G12: å¤–ã‚Œå€¤å‡¦ç†
        
        Alloy constraint: fact OutlierConstraints
        """
        print("\n" + "="*60)
        print("[G12] å¤–ã‚Œå€¤å‡¦ç†é–‹å§‹...")
        print("="*60)
        
        df = df.copy()
        
        # å¹´é½¢: 0-120æ­³ã®ç¯„å›²ã«åˆ¶é™
        if 'Age' in df.columns:
            df['Age'] = df['Age'].clip(0, 120)
            self.validator.validate_outliers(df, 'Age', 0, 120)
        
        # é‹è³ƒ: 0ä»¥ä¸Š
        if 'Fare' in df.columns:
            df['Fare'] = df['Fare'].clip(0, None)
            print(f"âœ… [G12] Fare: è² ã®å€¤ã‚’0ã«ã‚¯ãƒªãƒƒãƒ—")
        
        return df


class FeatureEngineer:
    """
    G2: ç‰¹å¾´é‡ç”Ÿæˆï¼ˆæ‹¡å¼µç‰ˆï¼‰
    
    å¯¾å¿œã™ã‚‹Alloyè¿°èª: G2_Achieved
    å¯¾å¿œã™ã‚‹KAOS Goal:
    - G21: ãƒ‰ãƒ¡ã‚¤ãƒ³çŸ¥è­˜ç‰¹å¾´é‡
    - G22: çµ±è¨ˆçš„ç‰¹å¾´é‡
    - G23: ç›¸äº’ä½œç”¨ç‰¹å¾´é‡
    - G24-G27: è¿½åŠ ç‰¹å¾´é‡ï¼ˆæ‹¡å¼µç‰ˆï¼‰
    """
    
    def __init__(self, validator: AlloyConstraintValidator):
        self.validator = validator
        self.new_features = []
    
    def create_features(self, df: pd.DataFrame) -> pd.DataFrame:
        """
        Alloy constraint: fact FeatureEngineeringRules (simplified)
        æ–°ç‰¹å¾´é‡ã¯å­˜åœ¨ã™ã‚‹ï¼ˆå…·ä½“çš„ãªç”Ÿæˆæ–¹æ³•ã¯å®Ÿè£…ãƒ¬ãƒ™ãƒ«ã§æ±ºå®šï¼‰
        
        æ‹¡å¼µç‰ˆ: ã‚ˆã‚Šå¤šãã®ç‰¹å¾´é‡ã‚’ç”Ÿæˆ
        """
        print("\n" + "="*60)
        print("[G2] ç‰¹å¾´é‡ã‚¨ãƒ³ã‚¸ãƒ‹ã‚¢ãƒªãƒ³ã‚°é–‹å§‹ï¼ˆæ‹¡å¼µç‰ˆï¼‰...")
        print("="*60)
        
        df = df.copy()
        
        # G21: ãƒ‰ãƒ¡ã‚¤ãƒ³çŸ¥è­˜ã«åŸºã¥ãç‰¹å¾´é‡
        if 'SibSp' in df.columns and 'Parch' in df.columns:
            df['FamilySize'] = df['SibSp'] + df['Parch'] + 1
            df['IsAlone'] = (df['FamilySize'] == 1).astype(int)
            self.new_features.extend(['FamilySize', 'IsAlone'])
            print(f"  ğŸ”§ FamilySize, IsAloneç”Ÿæˆï¼ˆãƒ‰ãƒ¡ã‚¤ãƒ³çŸ¥è­˜ï¼‰")
        
        # G22: çµ±è¨ˆçš„ç‰¹å¾´é‡
        if 'Age' in df.columns:
            df['Age_binned'] = pd.cut(
                df['Age'], 
                bins=[0, 12, 18, 60, 120],
                labels=['child', 'teen', 'adult', 'senior']
            )
            # ã‚«ãƒ†ã‚´ãƒªã‚«ãƒ«ã‚’æ•°å€¤ã«å¤‰æ›
            df['Age_binned_numeric'] = df['Age_binned'].cat.codes
            # ã‚«ãƒ†ã‚´ãƒªã‚«ãƒ«ç‰ˆã¯å‰Šé™¤ï¼ˆæ•°å€¤ç‰ˆã®ã¿ä½¿ç”¨ï¼‰
            df = df.drop('Age_binned', axis=1)
            self.new_features.append('Age_binned_numeric')
            print(f"  ğŸ”§ Age_binned_numericç”Ÿæˆï¼ˆçµ±è¨ˆçš„ï¼‰")
        
        # G23: ç›¸äº’ä½œç”¨ç‰¹å¾´é‡
        if 'Fare' in df.columns and 'FamilySize' in df.columns:
            df['Fare_per_person'] = df['Fare'] / df['FamilySize']
            self.new_features.append('Fare_per_person')
            print(f"  ğŸ”§ Fare_per_personç”Ÿæˆï¼ˆç›¸äº’ä½œç”¨ï¼‰")
        
        # ========================================
        # ğŸ‘‡ æ‹¡å¼µç‰ˆï¼šæ–°ã—ã„ç‰¹å¾´é‡ã‚’è¿½åŠ  ğŸ‘‡
        # ========================================
        
        # G24: Nameï¼ˆæ•¬ç§°ï¼‰ã‹ã‚‰ç‰¹å¾´é‡ç”Ÿæˆã€æ–°è¦ã€‘
        if 'Name' in df.columns:
            # æ•¬ç§°ã‚’æŠ½å‡ºï¼ˆMr., Mrs., Miss. ãªã©ï¼‰
            df['Title'] = df['Name'].str.extract(' ([A-Za-z]+)\.', expand=False)
            
            # æ•¬ç§°ã‚’ã‚°ãƒ«ãƒ¼ãƒ—åŒ–ï¼ˆå¸Œå°‘ãªæ•¬ç§°ã‚’ã¾ã¨ã‚ã‚‹ï¼‰
            title_mapping = {
                'Mr': 'Mr', 'Miss': 'Miss', 'Mrs': 'Mrs', 'Master': 'Master',
                'Dr': 'Rare', 'Rev': 'Rare', 'Col': 'Rare', 'Major': 'Rare',
                'Mlle': 'Miss', 'Countess': 'Rare', 'Ms': 'Miss',
                'Lady': 'Rare', 'Jonkheer': 'Rare', 'Don': 'Rare',
                'Dona': 'Rare', 'Mme': 'Mrs', 'Capt': 'Rare', 'Sir': 'Rare'
            }
            df['Title'] = df['Title'].map(title_mapping).fillna('Rare')
            
            # æ•¬ç§°ã‚’æ•°å€¤åŒ–
            df['Title_encoded'] = pd.factorize(df['Title'])[0]
            self.new_features.append('Title_encoded')
            print(f"  ğŸ”§ Title_encodedç”Ÿæˆï¼ˆãƒ‰ãƒ¡ã‚¤ãƒ³çŸ¥è­˜ãƒ»æ–°è¦ï¼‰")
        
        # G25: Cabinï¼ˆå®¢å®¤ï¼‰ã‹ã‚‰ç‰¹å¾´é‡ç”Ÿæˆã€æ–°è¦ã€‘
        if 'Cabin' in df.columns:
            # Cabinã®æœ€åˆã®æ–‡å­—ï¼ˆãƒ‡ãƒƒã‚­éšå±¤: A, B, C, D, E, F, Gï¼‰
            df['Cabin_letter'] = df['Cabin'].str[0].fillna('U')
            
            # CabinãŒã‚ã‚‹ã‹ã©ã†ã‹ï¼ˆç”Ÿå­˜ç‡ã«å½±éŸ¿ï¼‰
            df['Has_Cabin'] = df['Cabin'].notna().astype(int)
            
            # Cabin_letterã‚’æ•°å€¤åŒ–
            df['Cabin_letter_encoded'] = pd.factorize(df['Cabin_letter'])[0]
            
            self.new_features.extend(['Has_Cabin', 'Cabin_letter_encoded'])
            print(f"  ğŸ”§ Has_Cabin, Cabin_letter_encodedç”Ÿæˆï¼ˆãƒ‰ãƒ¡ã‚¤ãƒ³çŸ¥è­˜ãƒ»æ–°è¦ï¼‰")
        
        # G26: Sexï¼ˆæ€§åˆ¥ï¼‰ã‚’æ•°å€¤åŒ–ã€æ–°è¦ã€‘
        if 'Sex' in df.columns:
            df['Sex_encoded'] = df['Sex'].map({'male': 0, 'female': 1})
            self.new_features.append('Sex_encoded')
            print(f"  ğŸ”§ Sex_encodedç”Ÿæˆï¼ˆå‰å‡¦ç†ãƒ»æ–°è¦ï¼‰")
        
        # G27: Embarkedï¼ˆä¹—èˆ¹æ¸¯ï¼‰ã‚’æ•°å€¤åŒ–ã€æ–°è¦ã€‘
        if 'Embarked' in df.columns:
            df['Embarked_encoded'] = pd.factorize(df['Embarked'])[0]
            self.new_features.append('Embarked_encoded')
            print(f"  ğŸ”§ Embarked_encodedç”Ÿæˆï¼ˆå‰å‡¦ç†ãƒ»æ–°è¦ï¼‰")
        
        # ========================================
        # ğŸ‘† ã“ã“ã¾ã§æ–°ã—ã„ç‰¹å¾´é‡ ğŸ‘†
        # ========================================
        
        # Alloyåˆ¶ç´„æ¤œè¨¼: æ–°ç‰¹å¾´é‡ãŒå­˜åœ¨ã™ã‚‹
        self.validator.validate_new_features_exist(df, self.new_features)
        
        print(f"\nâœ… [G2] ç‰¹å¾´é‡ã‚¨ãƒ³ã‚¸ãƒ‹ã‚¢ãƒªãƒ³ã‚°å®Œäº†: {len(df.columns)}ã‚«ãƒ©ãƒ ")
        print(f"  ğŸ“Š æ–°è¦ç”Ÿæˆç‰¹å¾´é‡: {len(self.new_features)}å€‹")
        return df


class ModelTrainer:
    """
    G3: ãƒ¢ãƒ‡ãƒ«æ§‹ç¯‰
    
    å¯¾å¿œã™ã‚‹Alloyè¿°èª: G3_Achieved
    å¯¾å¿œã™ã‚‹KAOS Goal:
    - G31: ãƒ™ãƒ¼ã‚¹ãƒ©ã‚¤ãƒ³ãƒ¢ãƒ‡ãƒ«
    - G32: ãƒã‚¤ãƒ‘ãƒ¼ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿æœ€é©åŒ–
    - G33: ã‚¢ãƒ³ã‚µãƒ³ãƒ–ãƒ«ï¼ˆä»Šå›ã¯çœç•¥ï¼‰
    """
    
    def __init__(self, validator: AlloyConstraintValidator):
        self.validator = validator
        self.model = None
        self.hyperparameters = {}
    
    def set_hyperparameters(self, **kwargs):
        """
        G32: ãƒã‚¤ãƒ‘ãƒ¼ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿è¨­å®š
        
        Alloy constraint: fact HyperparameterConstraints
        """
        print("\n" + "="*60)
        print("[G32] ãƒã‚¤ãƒ‘ãƒ¼ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿è¨­å®š...")
        print("="*60)
        
        self.hyperparameters = kwargs
        
        # Alloyåˆ¶ç´„æ¤œè¨¼
        self.validator.validate_hyperparameters(self.hyperparameters)
    
    def train(self, X: pd.DataFrame, y: pd.Series) -> RandomForestClassifier:
        """
        G31: ãƒ¢ãƒ‡ãƒ«è¨“ç·´
        """
        print("\n" + "="*60)
        print("[G31] ãƒ¢ãƒ‡ãƒ«è¨“ç·´é–‹å§‹...")
        print("="*60)
        
        self.model = RandomForestClassifier(**self.hyperparameters, random_state=42)
        self.model.fit(X, y)
        
        print(f"âœ… [G31] RandomForestè¨“ç·´å®Œäº†")
        print(f"  ğŸ“Š ç‰¹å¾´é‡æ•°: {X.shape[1]}")
        print(f"  ğŸ“Š è¨“ç·´ãƒ‡ãƒ¼ã‚¿æ•°: {X.shape[0]}")
        
        return self.model
    
    def predict(self, X: pd.DataFrame) -> np.ndarray:
        """
        äºˆæ¸¬å®Ÿè¡Œ
        
        Alloy constraint: fact PredictionBinary
        """
        if self.model is None:
            raise ValueError("ãƒ¢ãƒ‡ãƒ«ãŒè¨“ç·´ã•ã‚Œã¦ã„ã¾ã›ã‚“")
        
        predictions = self.model.predict(X)
        
        # Alloyåˆ¶ç´„æ¤œè¨¼
        self.validator.validate_predictions_binary(predictions)
        
        return predictions


class ModelEvaluator:
    """
    G4: è©•ä¾¡ãƒ»æ”¹å–„ï¼ˆå®Ÿè£…ãƒ¬ãƒ™ãƒ«ã®ã¿ã€Alloyæ¤œè¨¼ãªã—ï¼‰
    
    æ³¨æ„: G4ã¯Alloyã§æ¤œè¨¼ã•ã‚Œã¦ã„ã¾ã›ã‚“
    å®Ÿè£…ãƒ¬ãƒ™ãƒ«ã§è¿½åŠ ã•ã‚ŒãŸæ©Ÿèƒ½ã§ã™
    """
    
    def evaluate(self, y_true: np.ndarray, y_pred: np.ndarray) -> Dict[str, float]:
        """
        ãƒ¢ãƒ‡ãƒ«è©•ä¾¡
        
        æ³¨æ„: Alloyæ¤œè¨¼ãªã—ï¼ˆG4ã¯å½¢å¼åŒ–å›°é›£ã®ãŸã‚ï¼‰
        """
        print("\n" + "="*60)
        print("[G4] ãƒ¢ãƒ‡ãƒ«è©•ä¾¡é–‹å§‹ï¼ˆAlloyæ¤œè¨¼ãªã—ï¼‰...")
        print("="*60)
        
        metrics = {
            'accuracy': accuracy_score(y_true, y_pred),
            'precision': precision_score(y_true, y_pred, zero_division=0),
            'recall': recall_score(y_true, y_pred, zero_division=0),
            'f1_score': f1_score(y_true, y_pred, zero_division=0)
        }
        
        # çµæœè¡¨ç¤º
        for metric_name, value in metrics.items():
            print(f"  ğŸ“Š {metric_name}: {value:.4f}")
        
        return metrics
    
    def cross_validate(self, model, X, y, cv=5) -> Dict[str, float]:
        """
        ã‚¯ãƒ­ã‚¹ãƒãƒªãƒ‡ãƒ¼ã‚·ãƒ§ãƒ³
        """
        print(f"\n[G4] {cv}åˆ†å‰²ã‚¯ãƒ­ã‚¹ãƒãƒªãƒ‡ãƒ¼ã‚·ãƒ§ãƒ³é–‹å§‹...")
        
        cv_scores = cross_val_score(model, X, y, cv=cv, scoring='accuracy')
        
        result = {
            'cv_mean': cv_scores.mean(),
            'cv_std': cv_scores.std()
        }
        
        print(f"  ğŸ“Š CVå¹³å‡: {result['cv_mean']:.4f} (+/- {result['cv_std']:.4f})")
        
        return result


class KagglePipeline:
    """
    å®Œå…¨ãªKaggleãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ï¼ˆG1-G3: Alloyæ¤œè¨¼æ¸ˆã¿ã€G4: å®Ÿè£…ã®ã¿ï¼‰
    æ‹¡å¼µç‰ˆï¼šã‚ˆã‚Šå¤šãã®ç‰¹å¾´é‡ã‚’ä½¿ç”¨
    """
    
    def __init__(self):
        self.validator = AlloyConstraintValidator()
        self.preprocessor = DataPreprocessor(self.validator)
        self.feature_engineer = FeatureEngineer(self.validator)
        self.trainer = ModelTrainer(self.validator)
        self.evaluator = ModelEvaluator()
    
    def execute(self, train_df: pd.DataFrame, test_df: pd.DataFrame, 
                target_col: str = 'Survived') -> Tuple[np.ndarray, Dict]:
        """
        å®Œå…¨ãªãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³å®Ÿè¡Œ
        
        Alloyè¿°èª: PracticalKagglePipeline (G1 âˆ§ G2 âˆ§ G3)
        """
        print("\n" + "="*70)
        print("ğŸš€ Kaggle AI-Augmentedå½¢å¼æ‰‹æ³•ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³å®Ÿè¡Œï¼ˆæ‹¡å¼µç‰ˆï¼‰")
        print("="*70)
        print(f"Alloyãƒ¢ãƒ‡ãƒ«: kaggle_competition_v3_final.als")
        print(f"æ¤œè¨¼æ¸ˆã¿ã‚´ãƒ¼ãƒ«: G1, G2, G3")
        print(f"å®Ÿè£…ãƒ¬ãƒ™ãƒ«: G4")
        print(f"ãƒãƒ¼ã‚¸ãƒ§ãƒ³: ç‰¹å¾´é‡æ‹¡å¼µç‰ˆ")
        print("="*70)
        
        # è¨“ç·´ãƒ»ãƒ†ã‚¹ãƒˆåˆ†é›¢æ¤œè¨¼
        self.validator.validate_train_test_separation(
            set(train_df.index),
            set(test_df.index)
        )
        
        # ã‚¿ãƒ¼ã‚²ãƒƒãƒˆä¿å­˜
        y_train = train_df[target_col]
        train_df = train_df.drop(target_col, axis=1)
        
        # G1: ãƒ‡ãƒ¼ã‚¿å‰å‡¦ç†
        print("\n" + "ğŸ”µ "*35)
        print("ã‚¹ãƒ†ãƒ¼ã‚¸1: G1 - ãƒ‡ãƒ¼ã‚¿å“è³ªä¿è¨¼ï¼ˆAlloyæ¤œè¨¼æ¸ˆã¿ï¼‰")
        print("ğŸ”µ "*35)
        
        train_df = self.preprocessor.handle_missing_values(train_df)
        train_df = self.preprocessor.handle_outliers(train_df)
        
        test_df = self.preprocessor.handle_missing_values(test_df)
        test_df = self.preprocessor.handle_outliers(test_df)
        
        # G2: ç‰¹å¾´é‡ã‚¨ãƒ³ã‚¸ãƒ‹ã‚¢ãƒªãƒ³ã‚°
        print("\n" + "ğŸŸ¢ "*35)
        print("ã‚¹ãƒ†ãƒ¼ã‚¸2: G2 - ç‰¹å¾´é‡ã‚¨ãƒ³ã‚¸ãƒ‹ã‚¢ãƒªãƒ³ã‚°ï¼ˆAlloyæ¤œè¨¼æ¸ˆã¿ãƒ»æ‹¡å¼µç‰ˆï¼‰")
        print("ğŸŸ¢ "*35)
        
        train_df = self.feature_engineer.create_features(train_df)
        test_df_fe = FeatureEngineer(self.validator)
        test_df = test_df_fe.create_features(test_df)
        
        # ç‰¹å¾´é‡é¸æŠï¼ˆæ‹¡å¼µç‰ˆï¼‰
        feature_cols = [
            # åŸºæœ¬ç‰¹å¾´é‡
            'Pclass', 'Age', 'SibSp', 'Parch', 'Fare',
            # æ—¢å­˜ã®ç”Ÿæˆç‰¹å¾´é‡
            'FamilySize', 'IsAlone', 'Fare_per_person', 'Age_binned_numeric',
            # æ–°è¦è¿½åŠ ç‰¹å¾´é‡
            'Title_encoded', 'Has_Cabin', 'Cabin_letter_encoded',
            'Sex_encoded', 'Embarked_encoded'
        ]
        
        # å­˜åœ¨ã™ã‚‹ç‰¹å¾´é‡ã®ã¿é¸æŠ
        feature_cols = [col for col in feature_cols if col in train_df.columns]
        
        print(f"\nğŸ“Š ä½¿ç”¨ç‰¹å¾´é‡: {len(feature_cols)}å€‹")
        print(f"  {', '.join(feature_cols)}")
        
        X_train = train_df[feature_cols]
        X_test = test_df[feature_cols]
        
        # G3: ãƒ¢ãƒ‡ãƒ«æ§‹ç¯‰
        print("\n" + "ğŸŸ¡ "*35)
        print("ã‚¹ãƒ†ãƒ¼ã‚¸3: G3 - ãƒ¢ãƒ‡ãƒ«æ§‹ç¯‰ï¼ˆAlloyæ¤œè¨¼æ¸ˆã¿ï¼‰")
        print("ğŸŸ¡ "*35)
        
        self.trainer.set_hyperparameters(
            max_depth=10,
            n_estimators=100,
            min_samples_split=5
        )
        model = self.trainer.train(X_train, y_train)
        
        # G4: è©•ä¾¡ï¼ˆå®Ÿè£…ãƒ¬ãƒ™ãƒ«ï¼‰
        print("\n" + "ğŸŸ£ "*35)
        print("ã‚¹ãƒ†ãƒ¼ã‚¸4: G4 - è©•ä¾¡ï¼ˆå®Ÿè£…ãƒ¬ãƒ™ãƒ«ã€Alloyæ¤œè¨¼ãªã—ï¼‰")
        print("ğŸŸ£ "*35)
        
        # è¨“ç·´ãƒ‡ãƒ¼ã‚¿ã§ã®è©•ä¾¡
        train_predictions = self.trainer.predict(X_train)
        train_metrics = self.evaluator.evaluate(y_train, train_predictions)
        
        # ã‚¯ãƒ­ã‚¹ãƒãƒªãƒ‡ãƒ¼ã‚·ãƒ§ãƒ³
        cv_results = self.evaluator.cross_validate(model, X_train, y_train, cv=5)
        
        # ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ã§ã®äºˆæ¸¬
        test_predictions = self.trainer.predict(X_test)
        
        # æœ€çµ‚æ¤œè¨¼
        print("\n" + "="*70)
        print("ğŸ” æœ€çµ‚Alloyåˆ¶ç´„æ¤œè¨¼ï¼ˆG1-G3ï¼‰")
        print("="*70)
        
        self.validator.validate_no_missing(train_df, "æœ€çµ‚è¨“ç·´ãƒ‡ãƒ¼ã‚¿")
        self.validator.validate_no_missing(test_df, "æœ€çµ‚ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿")
        self.validator.validate_predictions_binary(test_predictions)
        
        print("\n" + "="*70)
        print("âœ… ã™ã¹ã¦ã®Alloyåˆ¶ç´„ï¼ˆG1-G3ï¼‰ã‚’æº€ãŸã—ã¾ã—ãŸï¼")
        print("âœ… PracticalKagglePipelineé”æˆ")
        print("="*70)
        
        # çµæœã‚µãƒãƒªãƒ¼
        print("\n" + "ğŸ“Š "*35)
        print("çµæœã‚µãƒãƒªãƒ¼ï¼ˆæ‹¡å¼µç‰ˆï¼‰")
        print("ğŸ“Š "*35)
        print(f"  è¨“ç·´ç²¾åº¦: {train_metrics['accuracy']:.4f}")
        print(f"  CVç²¾åº¦: {cv_results['cv_mean']:.4f}")
        print(f"  ãƒ†ã‚¹ãƒˆäºˆæ¸¬æ•°: {len(test_predictions)}")
        print(f"  äºˆæ¸¬åˆ†å¸ƒ: 0={sum(test_predictions==0)}, 1={sum(test_predictions==1)}")
        
        return test_predictions, {
            'train_metrics': train_metrics,
            'cv_results': cv_results
        }


def main():
    """
    ã‚µãƒ³ãƒ—ãƒ«ãƒ‡ãƒ¼ã‚¿ã§ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³å®Ÿè¡Œ
    """
    print("="*70)
    print("Kaggle AI-Augmentedå½¢å¼æ‰‹æ³• - ã‚µãƒ³ãƒ—ãƒ«å®Ÿè¡Œï¼ˆæ‹¡å¼µç‰ˆï¼‰")
    print("="*70)
    
    # ã‚µãƒ³ãƒ—ãƒ«ãƒ‡ãƒ¼ã‚¿ç”Ÿæˆ
    print("\n[ãƒ‡ãƒ¼ã‚¿ç”Ÿæˆ] ã‚µãƒ³ãƒ—ãƒ«ãƒ‡ãƒ¼ã‚¿ã‚’ä½œæˆ...")
    np.random.seed(42)
    
    n_train = 100
    n_test = 50
    
    train_df = pd.DataFrame({
        'PassengerId': range(1, n_train + 1),
        'Survived': np.random.randint(0, 2, n_train),
        'Pclass': np.random.choice([1, 2, 3], n_train),
        'Age': np.random.normal(30, 15, n_train),
        'SibSp': np.random.poisson(0.5, n_train),
        'Parch': np.random.poisson(0.3, n_train),
        'Fare': np.random.exponential(30, n_train),
        'Embarked': np.random.choice(['C', 'Q', 'S'], n_train)
    })
    
    # æ„å›³çš„ã«æ¬ æå€¤ã‚’ä½œæˆ
    train_df.loc[np.random.choice(n_train, 10, replace=False), 'Age'] = np.nan
    train_df.loc[np.random.choice(n_train, 5, replace=False), 'Embarked'] = np.nan
    
    test_df = pd.DataFrame({
        'PassengerId': range(n_train + 1, n_train + n_test + 1),
        'Pclass': np.random.choice([1, 2, 3], n_test),
        'Age': np.random.normal(30, 15, n_test),
        'SibSp': np.random.poisson(0.5, n_test),
        'Parch': np.random.poisson(0.3, n_test),
        'Fare': np.random.exponential(30, n_test),
        'Embarked': np.random.choice(['C', 'Q', 'S'], n_test)
    }, index=range(n_train, n_train + n_test))
    
    # æ¬ æå€¤ã‚’ä½œæˆ
    age_missing_indices = test_df.index[np.random.choice(n_test, 5, replace=False)]
    test_df.loc[age_missing_indices, 'Age'] = np.nan
    
    print(f"  âœ… è¨“ç·´ãƒ‡ãƒ¼ã‚¿: {len(train_df)}è¡Œ")
    print(f"  âœ… ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿: {len(test_df)}è¡Œ")
    
    # ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³å®Ÿè¡Œ
    pipeline = KagglePipeline()
    predictions, results = pipeline.execute(train_df, test_df)
    
    # æå‡ºãƒ•ã‚¡ã‚¤ãƒ«ä½œæˆ
    submission = pd.DataFrame({
        'PassengerId': test_df['PassengerId'],
        'Survived': predictions
    })
    
    print("\n" + "="*70)
    print("ğŸ‰ ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³å®Œäº†ï¼ˆæ‹¡å¼µç‰ˆï¼‰")
    print("="*70)
    print(f"æå‡ºãƒ•ã‚¡ã‚¤ãƒ«: {len(submission)}è¡Œ")
    print(f"\n{submission.head(10)}")
    
    return submission, results


if __name__ == '__main__':
    submission, results = main()
